\documentclass[a4paper]{scrreprt}

\usepackage[german]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{ae}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}

\title{TGI Zusammenfassung}
\author{Fedor Scholz}
\maketitle

\tableofcontents
\vspace{1cm}

\chapter{Komplexitaetstheorie}



\section{Begriffe}

\subsection{Semi-Thue-System}
Ein Alphabet A und eine Menge von Produktionen P: (A, P).

\subsection{Kleene'scher Abschluss}
$L^* = \{w|w_1w_2...w_n$ $fuer$ $n \in N$ $und$ $w_i \in L$ $oder$ $w = \lambda\}$

\subsection{Relationen}
\subsubsection{Rechtsinvariante Relation}
Eine Relation R heißt rechtsinvariant, wenn $xRy \Rightarrow \forall z \in \Sigma^*: xzRyz$.

\subsubsection{Nerode-Relation}
$xR_Ly \Leftrightarrow \forall z \in \Sigma^*: (xz \in L \Leftrightarrow yz \in L)$.

\subsection{Pumping-Lemma}
\subsubsection{Pumping-Lemma fuer regulaere Sprachen}
Sei A eine regulaere Sprache, dann existiert eine Zahl p, sodass fuer alle $s \in A$ der Laenge $\ge p$ gilt: $s = xyz$ mit
\begin{itemize}
	\item $|y| > 0$ bzw. $y \neq \lambda$
	\item $|xy| \le p$
	\item $\forall i \ge 0: xy^iz \in A$
\end{itemize}

\subsubsection{Pumping-Lemma fuer kontextfreie Sprache}
Sei A eine kontextfreie Sprache, dann existiert eine Zahl p, sodass fuer alle $s \in A$ der Laenge $\ge p$ gilt: $s = uvxyz$ mit
\begin{itemize}
	\item $|vy| > 0$
	\item $|vxy| \le p$
	\item $\forall i \ge 0: uv^ixy^iz \in A$
\end{itemize}

\subsection{Chomsky-Normalform}
Eine Grammatik ist in Chomsky-Normalform, wenn jede Produktion eine der folgenden Formen hat:
\begin{itemize}
	\item $A \rightarrow BC$
	\item $A \rightarrow a$
	\item $S \rightarrow \lambda$ (falls $\lambda$ in der Sprache)
\end{itemize}

\subsection{Chomsky Typen}

\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Typ} & \textbf{Grammatik} & \textbf{Sprache} & \textbf{Entscheidbarkeit} & \textbf{Automaten}\\
\hline
\hline
0 & Beliebig formal & rekursiv aufzaehlbar & semientscheibar & Turingmaschine\\
\hline
1 & Kontextsensitiv & kontextsensitiv & entscheidbar & lin. beschr. Turingmaschine\\
\hline
2 & Kontextfrei & kontextfrei & entscheidbar & Kellerautomat\\
\hline
3 & Regulaer & rechtslinear & entscheidbar & Endlicher Akzeptor\\
\hline
\end{tabular}

\subsection{Linear beschraenkte Turingmaschine}
Nutzt nur den Teil des Bandes, welches von Anfang an beschrieben war.\\
M linear beschraenkt $\Rightarrow$ von M akzeptierte Sprache ist vom Typ Chomsky 1.

\subsection{Universelle Turingmaschine}
Nimmt \{0,1\}-Codierung einer Turingmaschine M und Eingabe entgegen, simuliert M und akzeptiert genau dann, wenn M akzeptiert.

\subsection{Berechenbare Funktion}
Eine Funktion f heißt berechenbar $\Leftrightarrow$ Es existiert eine Turingmaschine, die nach endlich vielen Schritten mit dem Ergebnis der Funktion auf dem Band haelt.

\subsection{Reduzierbarkeit}
\subsubsection{many-one-Reduzierbarkeit}
Eine Sprache A heißt many-one-reduzierbar auf die Sprache B ($A \le_m B$), falls eine berechenbare Funktion existiert mit:\\
$\forall w \in \Sigma^*: w \in A \Leftrightarrow f(w) \in B$\\
B heißt dann schwieriger als A.\\
$A \le_m B \Leftrightarrow$ (B entscheidbar $\Rightarrow$ A entscheidbar)

\subsubsection{Turingreduzierbarkeit}
$A \le_T B$, wenn es eine Orakelturingmaschine $TM^O$ gibt, die A entscheidet, wobei O ein Orakel fuer B ist.\\
Turingreduzierbarkeit $\neq$ many-one-Reduzierbarkeit

\subsection{Orakel}
Fuer eine Sprache B ist ein Orakel fuer B ein externes Geraet, welches fuer eine Turingmaschine entscheidet, ob ein Wort $w \in B$.
Eine Turingmaschine mit Zugriff auf ein Orakel heißt Orakelturingmaschine $TM^O$.

\subsection{Kolmogorow-Komplexitaet}
Fuer einen String ist $K(x)$ das kleinste d, sodass eine Turingmaschine mit Eingabe w x ausgibt und obige Codierung d Bits benoetigt.\\
$\exists c \forall x: K(x) \le |x| + c$\\
$\exists c \forall x: K(xx) \le K(x) + c$\\
Fuer jede Beschreibungssprache p existiert eine Konstante c mit $\forall x: K(x) \le K_p(x) + c$\\
Fuer jede Laenge n gibt es unkomprimierbare Strings der Laenge n.

\subsection{Speed-Up-Theorem}
Zu jeder Turingmaschine existiert eine Turingmascine, welche um einen konstanten Faktor schneller ist.

\subsection{Verifizierer}
Ein Verifizierer fuer eine Sprache A ist ein Algorithmus V mit $A = \{w|$ V akzeptiert $(w,c)$ fuer irgendein c\}\\
Ist die Laufzeit von V polynomial in $|w|$, so sagen wir A ist polynomial verfizierbar. Insbesondere ist $|c|$ dann polynomial in $|w|$.\\
c nennt man auch Zeugen oder Beweis.

\subsection{NP-Vollstaendigkeit}
Ein Problem A ist NP-vollstaendig, wenn $\forall B \in NP: B \le_p A$ und $A \in NP$.\\
Fuer L ist NP-vollstaendig zeigen wir: $L \in NP$ und $L \ge L'$ mit L' wurde schon als NP-vollstaendig bewiesen.\\\\
Laege ein NP-vollstaendiges Problem in P, wuerde $P = NP$ gelten.\\

\subsection{Fixed Parameter Tractable}
Eine Sprache $L \in NP$ nennen wir fixed parameter tractable, wenn es einen Parameter k gibt, sodass fuer konstantes k $x \in L$ in Polynomialzeit entscheidbar ist.

\subsection{Random Self Reducibility}
Gegeben sei ein dlog-Problem $(g, y)$, so koennen wir ein zufaelliges r wahlen und das Problem $(g, y*g^r) = (g, g^{x+r})$ loesen. Aus der Loesung x+r berechnen wir dann leicht x.




\subsection{Probleme}
\subsubsection{SAT} Erfuellbarkeitsproblem der Aussagenlogik fuer Formeln in konjunktiver Normalform. \textbf{NP-vollstaendig}.
\subsubsection{CLIQUE} Befindet sich in einem gegebenen Graphen eine Clique der Groeße k? \textbf{NP-vollstaendig}.
\subsubsection{VERTEX COVER} Hat der Graph einen Vertex Cover der Groeße k? \textbf{NP-vollstaendig}.
\subsubsection{HALT} $HALT = \{<M> w \in \Sigma^*|$ M haelt bei Eingabe w\}. \textbf{Nicht entscheidbar, aber semientscheidbar}. Ist die Turingmaschine linear beschraenkt, so ist das Halteproblem entscheidbar.
\subsubsection{Atm} $A_{TM} = \{<M> w \in \Sigma^*|$ M akzeptiert w\}. \textbf{Nicht entscheidbar, aber semientscheidbar}.
\subsubsection{MINtm} $MIN_{TM} = \{<M>|$ M ist minimale Turingmaschine\}. \textbf{Nicht entscheidbar}.
\subsubsection{PATH} $PATH = \{(G, s, t)|$ G ist eine gerichteter Graph und es gibt einen Pfad von Knoten s nach Knoten t\}. \textbf{$PATH \in P$}.
\subsubsection{RELPRIME} $RELPRIME = \{(x, y)|$ x und y sind teilerfremd\}. \textbf{$RELPRIME \in P$}
\subsubsection{Composite} $Composite = \{(x|x = p * q$ fuer $p,q \neq 1$\}, \textbf{$Composite \in P$}
\subsubsection{PRIMES} \textbf{$PRIMES \in P$}
\subsubsection{Klasse P} Klasse der Sprachen, die sich deterministisch in Polynomialzeit entscheiden lassen.
\subsubsection{Klasse NP} Klasse der Sprachen, die sich nichtdeterministisch in Polynomialzeit verifizieren lassen. Eine Sprache liegt genau dann in NP, wenn sie von einer nichtdeterministischen Turingmaschine in Polynomialzeit erkannt wird.
\subsubsection{Klasse NTIME (t(n))} $NTIME (t(n)) = \{L|$ L wird akzeptiert von einer nichtdeterministischen Turingmaschine in der Zeit $O(t(n))$\}



\section{Saetze}

\begin{itemize}
	\item Genau die regulaeren Sprachen lassen sich durch regulaere Ausdruecke beschreiben.
	\item Zu jedem nichtdeterministischen Automaten gibt es einen aequivalenten deterministischen.
	\item Jede kontextfreie Grammatik (Typ 2) laesst sich in Chomsky-Normalform bringen.
	\item der Cocke-Younger-Kasami-Algorithmus (CYK) entscheidet in $O(|P|*n^3)$, ob $w \in L(G)$, wobei G in Chomsky-Normalform vorliegen muss.
	\item Zu jeder kontextsensitiven Grammatik gibt es eine sprachaequivalente Grammatik deren Produktionen der Form
		\begin{itemize}
			\item $S \rightarrow a$
			\item $A \rightarrow a, a \in T$ (oder $S \rightarrow \lambda$)
			\item $\alpha B \beta \rightarrow \alpha\gamma\beta$ mit $\alpha,\beta,\gamma \in V^*,\gamma \neq \lambda, A \in V$
		\end{itemize}
	\item $R = RE \cap co-RE$: Eine Sprache ist entscheidbar, wenn sie und ihr Komplement semi-entscheidbar sind.
	\item Ist eine Sprache entscheidbar, so ist ihr Komplement auch entscheidbar.
	\item Das Postsche Korrespondenzproblem ist nicht entscheidbar.
	\item Es gibt eine berechenbare Funktion $q: \Sigma^* \rightarrow \Sigma^*$, wobei $\forall w \in \Sigma^*: q(w)$ ist Goedelnummer einer Maschine $P_w$, die w ausgibt und haelt, also $q(w) = <P_W>$.
	\item Es sei $t: \Sigma^* \rightarrow \Sigma^*$ eine berechenbare Funktion. Dann gibt es eine Turingmaschine T, fuer welche $t(<F>)$ die Goedelnummer eine Turingmaschine G ist, welche dieselbe Funktion wie F berechnet (aequivalent).
	\item Es sei $t(n) \ge n$, dann laesst sich jede $O(t(n)) time$ k-Band Turingmaschine in eine 1-Band Turingmaschine uebersetzen, die in $O(t^2(n)) time$ laeuft.
	\item Sei N eine nichtdeterministischen Turingmaschine, die ein Entscheider ist. Es bezeichne $f(n)$ die maximale Laenge des kuerzesten Pfades fuer alle Eingaben der Laenge n. Dann ist $f(n)$ die Laufzeit der nichtdeterministischen Turingmaschine N.
	\item $(L \le_p B$ und $B \in P) \Rightarrow L \in P$
\end{itemize}





\chapter{Informationstheorie}

\section{Begriffe}

\subsection{Information}
\begin{itemize}
	\item $I(p) = log_b(1/p) \ge 0$.
	\item $I(p_1*p_2) = I(p_1) + I(p_2)$.
	\item I sollte stetig sein.
\end{itemize}

\subsection{Entropie}
Die Entropie ist ein Maß, unter das nicht komprimiert werden kann. Entropie ist fuer Quellen definiert, die Kolmogorow-Komplexitaet ist auf einzelnen Strings definiert.\\

Die Entropie einer diskreten Zufallsvariable X ist\\

$H(X) = \sum_{x \in X}p(x)*log(1/p(x)) = EI(X) \ge 0$.

\subsubsection{Gemeinsame Entropie H(X,Y)}
Die gemeinsame Entropie der Zufallsvariablen X, Y mit der gemeinsamen Verteilung $p(X,Y)$ ist\\\\

$H(X,Y) = \sum_{x \in X}\sum_{y \in Y}p(x,y)*log(1/(p(x,y))$.

\subsubsection{Bedingte Entropie H(Y|X)}
Die bedingte Entropie der Zufallsvariable Y in Abhaengigkeit von X mit gemeinsamer Verteilung $p(x,y)$ ist\\\\

$H(Y|X) = \sum_{x \in X}p(x)*H(Y|X = x)\\
= -\sum_X p(x)*\sum_Y p(y|x)*log(p(y|x))\\
= -\sum_X \sum_Y p(x,y)*log(p(y|x))$.\\

Es gilt $H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y)$.\\
Es gilt $H((X,Y)|Z) = H(X|Z) + H(Y|X,Y)$.

\subsection{Transinformation I(X,Y)}
$I(X,Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$.

\subsection{Praefix Codes}
Ein Praefixcode (praefixfreier Code) ist eine Codierung, sodass fuer jedes Codewort $c = c_1...c_n$ kein $c_1...c_k, k < n$ ein Codewort ist (kein Praefix von c ist selbst ein Codewort).

\subsection{Shannon-Fero-Verfahren}
\begin{itemize}
	\item Sortiere die Symbole nach ihrer Haeufigkeit.
	\item Bestimme den Punkt, an dem die Reihe aufgeteilt werden kann, sodass die summierten Wahrscheinlichkeiten der beiden Teilfolgen moeglichst gleich sind.
	\item Haenge die beiden Teilfolgen als Blaetter an einer Wurzel.
	\item Rekursiv weiter: Ersetze jede Teilfolge durch den Baum, der beim Anwenden dieses Verfahrens entsteht, bis alle Teilfolgen einzelne Symbole sind.
	\item Der resultierende Baum ist ein Praefixcode.
\end{itemize}

\subsection{Huffman Code}
\begin{itemize}
	\item Erstelle aus jedem Symbol einen Baum, der genau dieses Symbol als einzigen Knoten hat.
	\item Wiederhole die folgenden Schritte solange, bis der Wald nur noch aus einem Baum besteht:
		\begin{itemize}
			\item Suche zwei Baeume, sodass die summierten Auftrittswahrscheinlichkeit aller enthaltenen Blaetter minimal wird.
			\item Erstelle einen neuen Baum, der eine neue Wurzel hat, an die die beiden ausgewaehlten Teilbaeume angehaengt werden.
		\end{itemize}
\end{itemize}
Der Baum (Praefixcode), der vom Huffman-Algorithmus erstellt wird, ist minimal.
Es bezeichne $l(\sigma)$ die Laenge des Pfades im Baum von Wurzel zu Blatt $\sigma$ und $p(\sigma)$ die Auftrittswahrscheinlichkeit von $\sigma$, dann miniert der Huffman-Code $E = \sum_{\sigma \in \Sigma}p(\sigma)*l(\sigma)$.

\subsection{Hamming Distanz}
Fuer $x,y \in Q^n$ ist $d(x,y) = \sum_{i=1}^n(1-\delta^{x_i, y_i} = \#\{i|i = 1,...,n,x_i \neq y_i\}$ die Hamming-Distanz zwischen x und y.

\subsection{Minimaldistanz}
Die Minimaldistanz eines nichttrivialen Block-Codes C ist $m(C) = min_{c_1,c_2 \in C, c_1 \neq c_2}d(c_1, c_2)$.\\\\
Fuer einen Code $C \subseteq Q^n$ heißt $R(C) = log(\#C)\log((\#Q)^n) = log(\#C)/(n*log(\#Q))$ die Informationsrate von C.\\
Je zwei Spalten der Parity-Check Matrix sind linear abhaengig (trivial) $\Rightarrow m(C) = 2 \Rightarrow$ Parity Codes koennen einen Bitfehler erkennen, aber keine Fehler korrigieren.

\subsection{Perfekte Codes}
Ein Code C mit ungerader Minimaldistanz $m(C)$ heißt perfekt, falls fuer jedes $x \in Q^n$ genau ein $c \in C$ gibt, sodass $wgt(x-c) \le (m(C)-1)/2$.

\subsection{Korrekturleistung}
Ein Block-Code C mit Minimaldistranz $m(C) = d$ kann entweder bis zu $d-1$ Fehler erkennen oder bis zu $\lfloor (d-1)/2 \rfloor$ Fehler zu korrigieren.

\subsection{Linearer Block-Code}
Ein linearer $[n,k]$-Block-Code C ist ein Untervektorraum von $\mathbb{F}_q^n$ der Dimension k.
Fuer lineare $[n,k]$-Codes C gilt $R(C) = k/n$.\\\\
Ist C ein linearer $[n,k]$-Code, so koennen wir C als Kern einer $\mathbb{F}^{(n-k)\times n}$-Matrix angeben: $C = Ker(H) = \{x \in \mathbb{F}^n|H*x = 0\}$.
H heißt Pruefmatrix oder Parity-Check-Matrix.\\
Beschreibung ueber Codierungsabbildung: Fuer $[n,k]$-Code C koennen wir $\mathbb{F}^{n \times k}$-Matrix G angeben, sodass $C = Bild(G) = \{y \in \mathbb{F}^n|\exists x \in \mathbb{F}^k:y = G*x\}$.
G bildet die Informationsworte auf Codeworte ab.\\\\
Fuer gegebene G,H gilt $H*G = 0$.
Fuer $x \in \mathbb{F}^n$ heißt $s = H*x$ das Fehlersyndrom von x.
s haengt nur von einem additiven Fehler, nicht aber vom Codewort selber ab. Ist $x = c+e$, so ist $H * x = H*(c+e) = H*c + H*e = H*e = s$.
Fuer gegebenes s heißt (falls eindeutig) das $e \in \mathbb{F}^n$ mit $wgt(e) = min\{wgt(x)|x \in \mathbb{F}^n, x \neq 0\}$ der Coset-Leader von s.

\subsection{Hamming-Metrik}
Fuer $x \in \mathbb{F}_q^n$ definieren wir die Hamming-Metrik (Hamming Gewicht, $L_0$-Norm) $wget(x) = d(x,0) = \sum_{i=1}^n(1-\delta_{x_i,0} = \#\{i|i=1,...,n,x_i \neq 0\}$.
Fuer Block-Codes gilt $d(x,y) = wget(x-y)$.

\subsection{Parity Codes}
Idee: Fuege einem Informationswort ein Bit hinzu, welches beschreibt, ob die Quersumme des Informationswortes gerade oder ungerade ist. Parity-Codes sind $[n, n-1]$-Codes.

\subsection{Rate}
$R(C) = (n-1)/n = 1-(1/n)$, d.h. $R(C) \rightarrow 1$ fuer $n \rightarrow \inf$.

\subsection{Hamming-Codes}
Hamming Codes sind $[2^k - 1, 2^k - k - 1]$-Codes, fuer die je zwei Spalten der Pruefmatrix linear unabhaengig sind.
$R(C) = 1-k/(2^k-1)$.
$m(C) = 2$.

\subsection{Perfekte Sicherheit}
Sei M der Klartext, B das Chiffrat, p(M) die Wahrscheinlichkeit, den Klartext zu raten. Dann heißt perfekte Sicherheit: $p(M|G) = p(M)$.

\subsection{Aequivokation}
$H(K|E) = -\sum_{E,K} p(K,E) * log(p(K|E))$.\\
Bei perfekter Sicherheit gilt: $H(M|E) = H(M)$.

\subsection{Probleme}
\subsubsection{COSET-WEIGHTS}
Gegeben: Pruefmatrix $H \in \mathbb{F}_2^{m \times n}$, ein Syndrom $s \in \mathbb{F}_2^m$ und Zahl k.\\
Frage: Gibt es ein E mit $wgt(e) \le k$, sodass $H*e = s$?\\\\
Das zugehoerige Suchproblem ist COSET-LEADER.\\
COSET-WEIGHTS ist NP-vollstaendig.
\subsubsection{SUBSPACE-WEIGHTS}
Gegeben: Pruefmatrix H und Zahl k.\\
Frage: Gibt es ein c mit $wgt(c) = k$, sodass $H*c = 0$?\\\\
SUBSPACE-WEIGHTS ist NP-vollstaendig.


\section{Saetze}
\begin{itemize}
	\item Wenn $\sigma_1$ und $\sigma_2$ die unwahrscheinlichsten Symbole sind, so gibt es einen optimalen Baum, in welchem $\sigma_1$ und $\sigma_2$ direkte Geschwister sind.
	\item Fuer eine Quelle mit der Rate R und einen verrauschten Kanal der Kapazitaet C, wobei $R \le C$, gibt es fuer jedes $\epsilon > 0$ eine Kanalcodierung, sodass die entstehende Uebertragungsrate $< \epsilon$.
	\item Block-Codes haben Woerter mit fester Laenge, Faltungscodes nicht.
\end{itemize}

\end{document}
